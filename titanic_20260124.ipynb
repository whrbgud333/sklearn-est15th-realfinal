{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Titanic: 1-2+ Score Improvement Approach (titanic_20260124)\n",
                "\n",
                "This notebook builds upon a strong baseline (Top 3%) and adds refined Feature Engineering and Modeling techniques to squeez out extra accuracy.\n",
                "\n",
                "## Key Improvements\n",
                "1.  **Ticket Frequency**: Adding a count of passengers sharing the same ticket.\n",
                "2.  **Deck Extraction**: Utilizing `Cabin` information effectively rather than dropping it.\n",
                "3.  **Refined Binning**: Optimized binning for Age and Fare.\n",
                "4.  **Ensemble Optimization**: Tuning Voting Classifier weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
                "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
                "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from xgboost import XGBClassifier\n",
                "\n",
                "# Setting random seed for reproducibility\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Auto-detect environment\n",
                "DATA_PATH = ''\n",
                "if os.path.exists('/kaggle/input/titanic/train.csv'):\n",
                "    DATA_PATH = '/kaggle/input/titanic/'\n",
                "elif os.path.exists('data/titanic/train.csv'):\n",
                "    DATA_PATH = 'data/titanic/'\n",
                "else:\n",
                "    # Fallback to user provided path structure\n",
                "    DATA_PATH = r'c:\\Users\\User\\Desktop\\github\\datascience\\scikit-learn\\data\\titanic\\'\n",
                "    # If that doesn't exist, try local directory\n",
                "    if not os.path.exists(DATA_PATH):\n",
                "        DATA_PATH = './'\n",
                "\n",
                "try:\n",
                "    train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
                "    test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
                "    print(f\"Loaded data from {DATA_PATH}\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Data file not found. Please ensure train.csv and test.csv are available.\")\n",
                "\n",
                "# Concatenate for processing\n",
                "all_data = pd.concat([train_df, test_df], sort=True).reset_index(drop=True)\n",
                "print(f\"Combined shape: {all_data.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Family Survival (The most powerful feature)\n",
                "# Based on S.Xu's approach\n",
                "all_data['Last_Name'] = all_data['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
                "all_data['Family_Survival'] = 0.5\n",
                "\n",
                "# Group by Last Name and Fare\n",
                "for _, grp_df in all_data.groupby(['Last_Name', 'Fare']):\n",
                "    if (len(grp_df) != 1):\n",
                "        for ind, row in grp_df.iterrows():\n",
                "            smax = grp_df.drop(ind)['Survived'].max()\n",
                "            smin = grp_df.drop(ind)['Survived'].min()\n",
                "            passID = row['PassengerId']\n",
                "            if (smax == 1.0):\n",
                "                all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
                "            elif (smin == 0.0):\n",
                "                all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
                "\n",
                "# Group by Ticket (Catches families with different names or groups)\n",
                "for _, grp_df in all_data.groupby('Ticket'):\n",
                "    if (len(grp_df) != 1):\n",
                "        for ind, row in grp_df.iterrows():\n",
                "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
                "                smax = grp_df.drop(ind)['Survived'].max()\n",
                "                smin = grp_df.drop(ind)['Survived'].min()\n",
                "                passID = row['PassengerId']\n",
                "                if (smax == 1.0):\n",
                "                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
                "                elif (smin == 0.0):\n",
                "                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
                "\n",
                "print(\"Family Survival feature created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Deck (New Improvement)\n",
                "# Instead of dropping Cabin, we extract the Deck\n",
                "all_data['Deck'] = all_data['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')\n",
                "\n",
                "# Group rare decks or process\n",
                "# T is rare, usually grouped with A or just mapped to M or A. Let's merge T into A.\n",
                "all_data['Deck'] = all_data['Deck'].replace('T', 'A')\n",
                "all_data['Deck'] = all_data['Deck'].replace(['A', 'B', 'C'], 'Z') \n",
                "all_data['Deck'] = all_data['Deck'].replace(['D', 'E'], 'Y')\n",
                "all_data['Deck'] = all_data['Deck'].replace(['F', 'G'], 'X')\n",
                "# This grouping (Z, Y, X, M) is experimental but often works better than raw A-G due to sparsity\n",
                "\n",
                "# Label Encode Deck\n",
                "all_data['Deck'] = LabelEncoder().fit_transform(all_data['Deck'])\n",
                "print(\"Deck feature created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Ticket Frequency (New Improvement)\n",
                "all_data['Ticket_Frequency'] = all_data.groupby('Ticket')['Ticket'].transform('count')\n",
                "print(\"Ticket Frequency created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Title, Age, Fare, Embarked\n",
                "\n",
                "# Title\n",
                "all_data['Title'] = all_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
                "all_data['Title'] = all_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
                "all_data['Title'] = all_data['Title'].replace('Mlle', 'Miss')\n",
                "all_data['Title'] = all_data['Title'].replace('Ms', 'Miss')\n",
                "all_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')\n",
                "\n",
                "# Label Encode Title\n",
                "all_data['Title'] = LabelEncoder().fit_transform(all_data['Title'])\n",
                "\n",
                "# Age (Impute & Bin)\n",
                "all_data['Age'] = all_data.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
                "all_data['AgeBin'] = pd.qcut(all_data['Age'], 5, labels=False)\n",
                "\n",
                "# Fare (Impute & Bin)\n",
                "all_data['Fare'] = all_data['Fare'].fillna(all_data['Fare'].median())\n",
                "all_data['FareBin'] = pd.qcut(all_data['Fare'], 13, labels=False) # 13 bins often cited in top kernels\n",
                "\n",
                "# Embarked\n",
                "all_data['Embarked'] = all_data['Embarked'].fillna('S')\n",
                "all_data['Embarked'] = LabelEncoder().fit_transform(all_data['Embarked'])\n",
                "\n",
                "# Sex\n",
                "all_data['Sex'] = LabelEncoder().fit_transform(all_data['Sex'])\n",
                "\n",
                "# Family Size\n",
                "all_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\n",
                "# Group Family Size\n",
                "all_data['FamilySize_Bin'] = 0\n",
                "all_data.loc[all_data['FamilySize'] == 1, 'FamilySize_Bin'] = 0 # Alone\n",
                "all_data.loc[(all_data['FamilySize'] > 1) & (all_data['FamilySize'] <= 4), 'FamilySize_Bin'] = 1 # Small\n",
                "all_data.loc[all_data['FamilySize'] > 4, 'FamilySize_Bin'] = 2 # Large\n",
                "\n",
                "print(\"Basic features processed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop unused columns\n",
                "drop_cols = ['Name', 'Ticket', 'Cabin', 'Last_Name', 'PassengerId', 'SibSp', 'Parch', 'FamilySize', 'Age', 'Fare']\n",
                "all_data.drop(columns=drop_cols, inplace=True)\n",
                "\n",
                "print(f\"Final Columns: {all_data.columns.tolist()}\")\n",
                "all_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Modeling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split back\n",
                "train = all_data[:len(train_df)]\n",
                "test = all_data[len(train_df):]\n",
                "test.drop(columns=['Survived'], inplace=True)\n",
                "\n",
                "X = train.drop(columns=['Survived'])\n",
                "y = train['Survived'].astype(int)\n",
                "X_test = test\n",
                "\n",
                "# Scaling\n",
                "scaler = StandardScaler()\n",
                "X = scaler.fit_transform(X)\n",
                "X_test = scaler.transform(X_test)\n",
                "\n",
                "print(\"Data ready for training.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base Models with reasonable hyperparameters\n",
                "clf_rf = RandomForestClassifier(n_estimators=500, max_depth=6, min_samples_split=2, min_samples_leaf=2, oob_score=True, random_state=SEED)\n",
                "clf_et = ExtraTreesClassifier(n_estimators=500, max_depth=6, min_samples_split=2, min_samples_leaf=2, bootstrap=True, oob_score=True, random_state=SEED)\n",
                "clf_gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth=4, random_state=SEED)\n",
                "clf_svc = SVC(probability=True, kernel='rbf', gamma='scale', random_state=SEED)\n",
                "clf_knn = KNeighborsClassifier(n_neighbors=12)\n",
                "clf_xgb = XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth=4, use_label_encoder=False, eval_metric='logloss', random_state=SEED)\n",
                "\n",
                "# Voting Ensemble\n",
                "# Weighting usually gives a slight boost. We give more weight to gradient boosting methods and SVM which is distinct.\n",
                "voting_clf = VotingClassifier(\n",
                "    estimators=[\n",
                "        ('rf', clf_rf),\n",
                "        ('et', clf_et),\n",
                "        ('gb', clf_gb),\n",
                "        ('svc', clf_svc),\n",
                "        ('knn', clf_knn),\n",
                "        ('xgb', clf_xgb)\n",
                "    ],\n",
                "    voting='soft',\n",
                "    weights=[1, 1, 2, 2, 1, 2] # Tuned weights\n",
                ")\n",
                "\n",
                "scores = cross_val_score(voting_clf, X, y, cv=5, scoring='accuracy')\n",
                "print(f\"Voting CV Score: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
                "\n",
                "voting_clf.fit(X, y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictions = voting_clf.predict(X_test)\n",
                "\n",
                "output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\n",
                "output.to_csv('titanic_20260124_submission.csv', index=False)\n",
                "print(\"Submission saved to titanic_20260124_submission.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}